`r opts_chunk$set(cache=F)`
# Simple species distribution model workflow

In this session we will perform a simple species distribution model workflow for the Solitary Tinamou (Tinamus solitarius).  
![Tinamus](figure/TinamusSolitariusSmit.jpg)
Illustration by Joseph Smit, 1895


## Objectives

In this session we will:

 1. Download and process some raster environmental data
 2. Process occurrence data from various sources
 3. Fit a Bayesian species distribution model using the observations and environmental data
 4. Predict across the landscape and write the results to disk as a geotif (for use in GIS, etc.)
 
 
```{r,echo=F}
# Load the libraries and set working directory
library(raster)
library (dismo)
library (maptools)
library(sp)
library(rasterVis)
library(rgeos)
library(rgdal)
library(rjags)
library(coda)
## set working directory
setwd("/home/user/ost4sem/exercise/SpatialAnalysisTutorials/workflow/Solitary_Tinamou")
```


## Data processing

### Import Gridded Environmental Data
Import some evironmental data (Climate, NPP, & Forest) and align it to a common grid
```{r}
# Check if data already exists
if(!file.exists("data/bio14_34_clip.tif"))
system("bash DataPrep.sh")
```

Read them in as a raster stack
```{r,ImportRaster}
env=stack(list.files(path = "data/", pattern="*_clip.tif$" , full.names = TRUE ))
## do some renaming for convenience
names(env)=sub("_34","",names(env))
names(env)=sub("_clip","",names(env))
names(env)[names(env)=="MOD17A3_Science_NPP_mean_00_12"]="npp"
## set missing value in npp
NAvalue(env[["npp"]])=65535
## get total % forest
forest=sum(env[[grep("consensus",names(env))]])
names(forest)="forest"
## add forest into the env stack
env=stack(env,forest)
##  List all available environmental data
names(env)
```

### Import point observations
Download point data of occurrences from the Global Biodiversity Information Facility (GBIF) dataset 
```{r}
gbif_points = gbif('Tinamus' , 'solitarius' , download=T , geo=T)
gbif_points=gbif_points[!is.na(gbif_points$lat),]
```
Import the ebird points
```{r}
ebird = read.table("data/lat_long_ebd.txt" ,header = TRUE  )
```

Import a presence-absence shapefile from park checklists.
```{r}
parks=readOGR("data/","protected_areas")
## Many of the parks with no observered presences were recorded as NA in the "Presence" Column. Replace them with 0s.
parks$Presence[is.na(parks$Presence)]=0
## generate an 'absence' dataset by sampling from the parks with no observed presences
nulls=coordinates(spsample(parks[parks$Presence==0,],25,type="stratified"))
```


Import IUCN expert range map
```{r}
tin_range=readOGR("data/","iucn_birds_proj")
tin_range=spTransform(tin_range,CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"))
```

Build a combined dataset (with source, presence/absence, and coordinates)
```{r}
points=rbind.data.frame(
  data.frame(src="gbif",obs=1,lat=gbif_points$lat,lon=gbif_points$lon),
  data.frame(src="ebird",obs=1,lat=ebird$LATITUDE,lon=ebird$LONGITUDE),
  data.frame(src="parks",obs=0,lat=nulls[,"x2"],lon=nulls[,"x1"])
)
## turn it into a spatial dataframe and define projection
coordinates(points)=c('lon','lat')
projection(points)="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"

## Create a combined src_presence field for easy plotting
points$type=paste(points$src,points$obs,sep="_")
```


Import a world country boundary to ground the map
```{r}
World  = readShapePoly ("data/world_country_admin_boundary_shapefile_with_fips_codes.shp")
projection(World)="+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
```

As we saw before, there are a few points just outside the range, but those in the ocean are most likely wrong.  Let's add the distance to the range polygon as a way to filter the observations.  First we need a equidistant projection to do the calculation
```{r}
dproj=CRS("+proj=eqc +lat_ts=0 +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +a=6371007 +b=6371007 +units=m +no_defs") 
points$dist=gDistance(spTransform(points,dproj),spTransform(tin_range,dproj),byid=T)[1,]
## that adds 'distance' (in meters) from each point to the polygon
## so some points are > 2000km from the range, let's drop any more than 10km
points=points[points$dist<10000,]
```

Check out the data in a plot
```{r}
spplot(points,zcol="type",pch=1:3,col.regions=c("red","red","black"))+
  layer(sp.polygons(parks,col=NA,fill=ifelse(parks$Presence==0,"black","red")),under=T)+
  layer(sp.polygons( World))+
  layer(sp.polygons(tin_range,fill="grey"),under=T)
```



Variable selection is tricky business and we're not going to dwell on it here... We'll use the following variables
```{r}
vars=c("bio5","bio6","bio13","bio14","npp","forest")
```
[Worldclim "BIO" variables](http://www.worldclim.org/bioclim)

 * BIO5 = Max Temperature of Warmest Month
 * BIO6 = Min Temperature of Coldest Month
 * BIO13 = Precipitation of Wettest Month
 * BIO14 = Precipitation of Driest Month

To faciliate model fitting and interpretation, let's scale the environmental data
```{r, scaledata}
senv=scale(env[[vars]])
## Make a plot to explore the data
levelplot(senv,col.regions=rainbow(100,start=.2,end=.9),cuts=99)
```

Add the (scaled) environmental data to each point
```{r}
pointsd=extract(senv,points,sp=F)
## create single data.frame to hold all data for modelling
pointsd2=data.frame(obs=points$obs,pointsd)
```

## Fit a simple GLM to the data
```{r}
m1=glm(obs~bio5+bio6+bio13+bio14+npp+forest,data=pointsd2,family="binomial")
summary(m1)
```

## Simple Bayesian Distribution Model
```{r, fitmodel,tidy=FALSE} 
## create the data object 
jags.data <- list(N.cells = nrow(pointsd2),
                  obs=points$obs,
                  X=data.frame(1,pointsd2[,vars]),
                  nBeta=length(vars)+1)
 
# define the model
cat("
    model
{
    # priors
    for (l in 1:nBeta) {
    beta[l] ~ dnorm(0,0.01)
    }
    
    # likelihood
    for(i in 1:N.cells)
{
    # The observation as the result of a bernoulli outcome
    obs[i] ~ dbern(p[i])
    # logit transformation
    p[i]<-1/(1+exp(-lp.lim[i]))
    # Alternatively, could use the built-in function
    # logit(p[i])<-lp.lim[i]
    # 'stabilize' the logit to prevent hitting size limits
    lp.lim[i]<-min(999,max(-999,lp[i])) 
    }
    # The regression 
    # (using matrix notation rather than lp<-beta1+beta2*X1, etc)
    lp <- X%*%beta
    }
    ", file="model.txt")

params <- c("beta","p")

jm <- jags.model("model.txt",
                 data = jags.data,
                 n.chains = 3,
                 n.adapt = 2000)
```

The model has been defined and an initial adaptive run of 2000 iterations complete.  Let's take some samples.
```{r, samplemodel}
jm.sample <- jags.samples(jm, variable.names = params, n.iter = 5000, thin = 5)
```

Extract the posterior samples and convert to `mcmc.list` objects for plotting/summaries
```{r}
ps.beta=as.mcmc.list(jm.sample$beta)
ps.p=as.mcmc.list(jm.sample$p)
```

### Check Convergence
```{r}
xyplot(ps.beta, main="Beta",strip=strip.custom(factor.levels=c("intercept",vars)))
gelman.diag(ps.beta,confidence = 0.95,autoburnin=F,multivariate=T)

```


## Summarize the posterior betas
```{r}
densityplot(ps.beta, main="Posterior Distributions",
            strip=strip.custom(factor.levels=c("intercept",vars)),
            scales=list(relation="same"),layout=c(1,7))+
  layer(panel.abline(v=0))
HPDinterval(ps.beta[[1]], prob = 0.95)

```

## Predict model to the grid

```{r,predictmodel}
## First subset area to speed up predictions
pext=extent(c(-50,-48,-26.5,-24))
penv=crop(senv,pext)

## if you want to make predictions for the full grid, run this line:
#penv=senv

## Calculate posterior estimates of p(occurrence) for each cell
## This extracts the posterior coefficients, performs the regression, 
## calculates the quantiles, and takes the inverse logit to get p(occurrence)

## niter will use a reduced number of posterior samples to generate the summaries
pred=calc(penv,function(x,niter=30) {
  mu1=apply(apply(ps.beta[[1]][1:niter,],1,function(y) y*c(1,x)),2,sum,na.rm=T)
  mu2=quantile(mu1,c(0.025,0.5,0.975),na.rm=T)  
  p=1/(1+exp(-mu2))
  return(p)
})
names(pred)=c("Lower_CI_2.5","Median","Upper_CI_97.5")
## Write out the predictions
writeRaster(pred,file="Prediction.tif",overwrite=T)
```

Plot the predictions
```{r}
levelplot(pred,col.regions=rainbow(100,start=.2,end=.9),cuts=99,margin=F)+
  layer(sp.polygons(tin_range,lwd=2))+
  layer(sp.points(points[points$obs==0,],pch="-",col="black",cex=8,lwd=4))+ #add absences
  layer(sp.points(points[points$obs==1,],pch="+",col="black",cex=4,lwd=4))    #add presences
```

# Summary

In this script we have illustrated a complete workflow, including:

 1. Calling a BASH script (including GDAL Functions) from R to perform data pre-processing
 2. Running a (simple) Bayesian Species Distribution Model using rjags
 3. Making spatial predictions from model posteriors
 4. Writing results to disk as a geotif (for use in GIS, etc.)
 
